import streamlit as st
from llm import LLMClient

# Set the page title and icon
st.set_page_config(page_title="PyDough LLM Demo", page_icon="bodo_icon.png")

# Logo
st.image("logo.png", width=150, use_container_width=False)

# ---------------------- APP HEADER ----------------------
st.title("PyDough LLM Demo")

st.markdown(
    """
    This interactive demo allows you to generate **PyDough queries** from natural language instructions.
    Simply enter a query and explore the results in a **conversational format**.
    """,
    unsafe_allow_html=True,
)

# ---------------------- DATABASE DIAGRAM ----------------------
@st.dialog("üìä TPCH Database Diagram", width="large")
def show_db_diagram():
    st.image("db_diag.png", use_container_width=True)
    
if st.button("View TPCH Diagram üìä"):
    show_db_diagram()

st.markdown(
    """
    You can inspect:
    - **Code**: The PyDough query generated by the LLM.
    - **Full Explanation**: A detailed explanation of how the query works.
    - **DataFrame**: The table containing the query results.
    - **SQL**: The SQL equivalent of the PyDough query.
    - **Exception**: Stores any errors encountered while executing the query.
    - **Base Prompt**: The initial instruction given to the LLM to generate the query.
    - **Cheat Sheet**: A reference guide or example queries to help the LLM structure responses.
    - **Knowledge Graph**: The metadata structure that informs the LLM about available collections and relationships.
    """,
    unsafe_allow_html=True,
)

# ---------------------- EXAMPLES MODAL ----------------------
@st.dialog("üí° Example Queries for TPCH", width="large") 
def show_examples():
    st.write("You can **copy** any of the examples by hovering on it and clicking at the copy button on the right side. Then paste it on the query text-field and try it out!")
    
    examples = [
        "Total customers & suppliers per nation, ordered by nation name.",
        "Top 5 nations with most customer orders in 1995.",
        "Region with highest total order value in 1996.\n\nSUM(extended_price * (1 - discount))",
        "Top 3 regions with most distinct customers.",
        "Customers & order count in 1995 (Europe) with balance > $700.",
        "Top 10 customers who bought 'green' products in 1998 (with quantity & address).",
        "Customers with more orders in 1995 than 1994.",
        "Avg. order value per nation.\n\nSUM(extended_price * quantity)",
        "Total revenue per customer in 1994.\n\nSUM(extended_price * (1 - discount))",
        "Customer key, name & revenue in 1994.\n\nSUM(extended_price * (1 - discount))",
        "Customers ending in zero with 30-lowest balances.",
        "Customers with >10 orders, showing name & total order count.",
        "Orders from 1998 with total price > $100, sorted by price.",
        "Customers who ordered in 1996 but not in 1997, with email & total spent (> $200)."
    ]

    for example in examples:
        st.code(example, language="")

col1, col2 = st.columns([0.85, 1.28])  
with col1:
    st.markdown('<p style="margin-top:10px;">Don\'t know what to write? Check out our</p>', unsafe_allow_html=True)
with col2: 
    if st.button("Examples"):
        show_examples()

import streamlit as st
from llm import LLMClient

# ---------------------- CONVERSATIONAL INTERFACE ----------------------
st.header("Query Interface")

# Initialize session state for messages and dropdown selection
if "messages" not in st.session_state:
    st.session_state.messages = []
if "selected_output" not in st.session_state:
    st.session_state.selected_output = {}

# Display chat history
for idx, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

    # If this is an assistant message with query results, show a dropdown
    if message["role"] == "assistant" and "query_result" in message:
        result = message["query_result"]

        if not result or (not result.code and not result.full_explanation and not result.df):
            st.error("‚ö†Ô∏è No valid response received. Please try again.")
            continue  # Skip displaying dropdown

        dropdown_key = f"dropdown_{idx}"

        # Restore previous selection or default to "Code"
        selected_output = st.session_state.selected_output.get(dropdown_key, "Code")

        # Create dropdown and store selection
        selected_output = st.selectbox(
            "Your answer is ready! Select the result info you want below:",
            ["Code", "Full Explanation", "DataFrame", "SQL", "Exception", 
              "Base Prompt", "Cheat Sheet", "Knowledge Graph"],
            key=dropdown_key,
            index=["Code", "Full Explanation", "DataFrame", "SQL", "Exception",
                    "Base Prompt", "Cheat Sheet", "Knowledge Graph"].index(selected_output)
        )

        st.session_state.selected_output[dropdown_key] = selected_output

        # Display selected output
        response_content = ""
        if selected_output == "Code":
            response_content = f"```python\n{result.code}\n```"
        elif selected_output == "Full Explanation":
            response_content = result.full_explanation
        elif selected_output == "DataFrame":
            if hasattr(result, "df") and result.df is not None:
                st.dataframe(result.df)  # ‚úÖ Correct DataFrame display
            else:
                st.warning("‚ö†Ô∏è No DataFrame available.")
            continue  # Skip displaying markdown to avoid double output
        elif selected_output == "SQL":
            response_content = f"```sql\n{result.sql}\n```"
        elif selected_output == "Exception":
            response_content = result.exception or "No exception found."
        elif selected_output == "Base Prompt":
            response_content = result.base_prompt or "No base prompt found."
        elif selected_output == "Cheat Sheet":
            response_content = result.cheat_sheet or "No cheat sheet found."
        elif selected_output == "Knowledge Graph":
            response_content = result.knowledge_graph or "No knowledge graph found."

        st.markdown(response_content)

# ---------------------- USER INPUT ----------------------
if query := st.chat_input("Ask a query about the TPCH database..."):

    st.session_state.messages.append({"role": "user", "content": query})

    with st.chat_message("user"):
        st.markdown(query)

    try:
        client = LLMClient()
        result = client.ask(query)

        # Check if result is empty
        if not result or (not result.code and not result.full_explanation and not result.df):
            st.error("‚ö†Ô∏è No valid response received. Please try again.")
        else:
            # Store result in chat history with a placeholder for dropdown
            st.session_state.messages.append({
                "role": "assistant", 
                "content": "Your answer is ready! Select the result info you want below:",  
                "query_result": result  
            })

            # Refresh to show new dropdown immediately
            st.rerun()

    except Exception as e:
        st.error(f"‚ùå Error running query: {e}")

# ---------------------- RESET BUTTON ----------------------
if st.button("Reset Conversation"):
    st.session_state.messages = []
    st.session_state.selected_output = {}
    st.rerun()


