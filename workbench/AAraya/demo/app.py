import streamlit as st
from llm import LLMClient

# Set the page title and icon
st.set_page_config(page_title="PyDough LLM Demo", page_icon="bodo_icon.png")

# Logo
st.image("logo.png", width=150, use_container_width=False)

# ---------------------- APP HEADER ----------------------
st.title("PyDough LLM Demo")

st.markdown(
    """
    This interactive demo allows you to generate **PyDough queries** from natural language instructions.
    Simply enter a query and explore the results in a **conversational format**.
    """,
    unsafe_allow_html=True,
)

# ---------------------- DATABASE DIAGRAM ----------------------
@st.dialog("ðŸ“Š TPCH Database Diagram", width="large")
def show_db_diagram():
    st.image("db_diag.png", use_container_width=True)
    
if st.button("View TPCH Diagram ðŸ“Š"):
    show_db_diagram()

st.markdown(
    """
    You can inspect:
    - **Code**: The PyDough query generated by the LLM.
    - **Full Explanation**: A detailed explanation of how the query works.
    - **DataFrame**: The table containing the query results.
    - **SQL**: The SQL equivalent of the PyDough query.
    - **Exception**: Stores any errors encountered while executing the query.
    - **Original Question**: The natural language question input by the user.
    - **Base Prompt**: The initial instruction given to the LLM to generate the query.
    - **Cheat Sheet**: A reference guide or example queries to help the LLM structure responses.
    - **Knowledge Graph**: The metadata structure that informs the LLM about available collections and relationships.
    """,
    unsafe_allow_html=True,
)

# ---------------------- EXAMPLES MODAL ----------------------
@st.dialog("ðŸ’¡ Example Queries for TPCH", width="large") 
def show_examples():
    st.write("You can **copy** any of the examples by hovering on it and clicking at the copy button on the right side. Then paste it on the query text-field and try it out!")
    
    examples = [
        "Total customers & suppliers per nation, ordered by nation name.",
        "Top 5 nations with most customer orders in 1995.",
        "Region with highest total order value in 1996.\n\nSUM(extended_price * (1 - discount))",
        "Top 3 regions with most distinct customers.",
        "Customers & order count in 1995 (Europe) with balance > $700.",
        "Top 10 customers who bought 'green' products in 1998 (with quantity & address).",
        "Customers with more orders in 1995 than 1994.",
        "Avg. order value per nation.\n\nSUM(extended_price * quantity)",
        "Total revenue per customer in 1994.\n\nSUM(extended_price * (1 - discount))",
        "Customer key, name & revenue in 1994.\n\nSUM(extended_price * (1 - discount))",
        "Customers ending in zero with 30-lowest balances.",
        "Customers with >10 orders, showing name & total order count.",
        "Orders from 1998 with total price > $100, sorted by price.",
        "Customers who ordered in 1996 but not in 1997, with email & total spent (> $200)."
    ]

    for example in examples:
        st.code(example, language="")

col1, col2 = st.columns([0.85, 1.28])  
with col1:
    st.markdown('<p style="margin-top:10px;">Don\'t know what to write? Check out our</p>', unsafe_allow_html=True)
with col2: 
    if st.button("Examples"):
        show_examples()

# ---------------------- CONVERSATIONAL INTERFACE ----------------------
st.header("Query Interface")


if "messages" not in st.session_state:
    st.session_state.messages = []


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User input
if query := st.chat_input("Ask a query about the TPCH database..."):

    st.session_state.messages.append({"role": "user", "content": query})


    with st.chat_message("user"):
        st.markdown(query)


    try:
        client = LLMClient()
        result = client.ask(query)

        # Dropdown for selecting output type
        selected_output = st.selectbox(
            "Select what to view:",
            ["Code", "Full Explanation", "DataFrame", "SQL", "Exception", 
             "Original Question", "Base Prompt", "Cheat Sheet", "Knowledge Graph"],
            key=f"dropdown_{len(st.session_state.messages)}",
        )

        # Generate response content
        response_content = ""
        if selected_output == "Code":
            response_content = f"```python\n{result.code}\n```"
        elif selected_output == "Full Explanation":
            response_content = result.full_explanation
        elif selected_output == "DataFrame":
            response_content = result.df 
        elif selected_output == "SQL":
            response_content = f"```sql\n{result.sql}\n```"
        elif selected_output == "Exception":
            response_content = result.exception
        elif selected_output == "Original Question":
            response_content = result.original_question
        elif selected_output == "Base Prompt":
            response_content = result.base_prompt
        elif selected_output == "Cheat Sheet":
            response_content = result.cheat_sheet
        elif selected_output == "Knowledge Graph":
            response_content = result.knowledge_graph

        # Store response in chat history
        st.session_state.messages.append({"role": "assistant", "content": response_content})

        # Display response
        with st.chat_message("assistant"):
            st.markdown(response_content)

    except Exception as e:
        st.error(f"Error running query: {e}")

# ---------------------- RESET BUTTON ----------------------
if st.button("Reset Conversation"):
    st.session_state.messages = []
    st.rerun()

